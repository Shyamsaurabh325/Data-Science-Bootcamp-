{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a762d984-709a-4796-9801-42bf48d2d554",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "#### Random Forest is an ensemble learning method that combines multiple decision trees trained on random subsets of data and features, using bagging and majority voting to improve accuracy, reduce overfitting, and handle complex datasets effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3673c65a-5db4-4005-9a59-2b19634b6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Generate synthetic data\n",
    "x, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "# Split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(x_test)\n",
    "print(f\"Accuracy: {rf.score(x_test, y_test)}\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e715d7-e4de-46c9-8c04-ef2cf4548b06",
   "metadata": {},
   "source": [
    "# There are for types of Boosting Algoritham\n",
    "#### 1.XGboost\n",
    "#### 2.Ligst GBM(Gradient boosting machine)\n",
    "#### 3.AdaBoost\n",
    "#### 4.Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1a276-3b6b-4213-929b-6b0ae5475cd4",
   "metadata": {},
   "source": [
    "## 1. XGBoost\n",
    "#### XGBoost (Extreme Gradient Boosting) is a scalable and efficient implementation of gradient boosting that optimizes performance through regularization, parallel processing, and handling missing values effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbe5130f-f9a9-4484-b3cc-6d3083921c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Convert data into DMatrix format (optimized for XGBoost)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # For Binary Classification\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 60\n",
    "}\n",
    "# Train the model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "# Predictions\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"XGBoost Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bbf1d-bf89-4e06-935b-b6c028b36835",
   "metadata": {},
   "source": [
    "## 2. Ada Boost\n",
    "#### AdaBoost (Adaptive Boosting) is an ensemble learning method that improves model performance by sequentially training weak learners, with each focusing more on the errors of the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e717ebf2-e516-4093-8c6d-0881d968889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy:0.8633333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Initialize AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=50 , random_state=42)\n",
    "ada.fit(x_train,y_train)\n",
    "# Predict and evaluate\n",
    "y_pred=ada.predict(x_test)\n",
    "print(f\"AdaBoost Accuracy:{ada.score(x_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6dff3-0667-4e81-87a6-ddf7a1e32cc4",
   "metadata": {},
   "source": [
    "## 3.Gradient Descent\n",
    "#### Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent based on the gradient of the function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f9a1fa1-4e80-4b91-8680-ebb4f73ba873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope:0.62,Intercept:0.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Simple dataset\n",
    "x=np.array([1,2,3,4,5])\n",
    "y=np.array([5,7,9,11,13])\n",
    "\n",
    "# Initialize parameters\n",
    "m=b=0 # Slope and intercept\n",
    "learning_rate = 0.01\n",
    "epochs=1000\n",
    "# Gradient Descent Algoritham\n",
    "for _ in range (epochs):\n",
    "    y_pred = m*x+b # Predicted vlaue\n",
    "D_m=(-2/len(x))* sum(x*(y-y_pred)) # Derivative Wrt m\n",
    "D_b=(-2/len(x))* sum(y-y_pred) # Derivative wrt b\n",
    "m-=learning_rate*D_m # update m\n",
    "b-=learning_rate *D_b # update b\n",
    "print(f\"slope:{m},Intercept:{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7305aea-823c-40a7-8e1e-a0faf0df1380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
